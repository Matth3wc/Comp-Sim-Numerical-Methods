{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2 — Richardson Extrapolation with Application to Numerical Derivatives. \n",
    "\n",
    "**Richardson Extrapolation with Application to Numerical Derivatives**\n",
    "\n",
    "Originally devised by L.\\,F.\\ Richardson for weather prediction, Richardson extrapolation is a general technique to eliminate leading error terms. Here we apply it to obtain higher-accuracy numerical differentiation schemes. \n",
    "\n",
    "\n",
    "**Central-difference error structure**\n",
    "\n",
    "Starting from the three-point central difference,\n",
    "\\begin{equation}\n",
    "f'(x) \\;=\\; \\frac{f(x+h)-f(x-h)}{2h}\\;-\\;\\frac{1}{6}h^{2}f^{(3)}(x)\\;+\\;\\cdots \\tag{2.1} % \n",
    "\\end{equation}\n",
    "and using the same formula at step size $2h$,\n",
    "\\begin{equation}\n",
    "f'(x) \\;=\\; \\frac{f(x+2h)-f(x-2h)}{4h}\\;-\\;\\frac{4}{6}h^{2}f^{(3)}(x)\\;+\\;\\cdots \\tag{2.2} % \n",
    "\\end{equation}\n",
    "we note that the leading $O(h^{2})$ error in \\eqref{2.1} is four times smaller than in \\eqref{2.2}.\n",
    "\n",
    "Subtracting $\\frac{1}{4} \\times$ equation 2.2 from equation 2.1 eliminates the $h^{2}$ term and yields the five-point central difference:\n",
    "\\begin{equation}\n",
    "f'(x) \\;=\\; \\frac{\\,f(x-2h)-f(x+2h)+8f(x+h)-8f(x-h)\\,}{12h}\n",
    "\\;+\\;\\frac{1}{30}h^{4}f^{(5)}(x)\\;+\\;\\cdots \\tag{2.3} % \n",
    "\\end{equation}\n",
    "\n",
    "\\\n",
    "*Recursive Richardson table for derivatives*\n",
    "\n",
    "Define the shorthand\n",
    "\\begin{equation}\n",
    "D_1(h) \\;:=\\; \\frac{f(x+h)-f(x-h)}{2h}, \n",
    "\\qquad\n",
    "D_1(2h) \\;:=\\; \\frac{f(x+2h)-f(x-2h)}{4h}, \\tag{2.4--2.5} % \n",
    "\\end{equation}\n",
    "so that $D_1$ has even-power error terms with leading $O(h^{2})$.\n",
    "\n",
    "One step of Richardson extrapolation removes that leading term:\n",
    "\\begin{equation}\n",
    "D_2(h) \\;:=\\; D_1(h)\\;-\\;\\frac{D_1(2h)-D_1(h)}{2^{2}-1}\n",
    "\\;=\\;\\frac{4D_1(h)-D_1(2h)}{3}, \\tag{2.6} % \n",
    "\\end{equation}\n",
    "leaving an $O(h^{4})$ error.\n",
    "\n",
    "Repeating the idea on $D_2$ removes the $O(h^{4})$ term:\n",
    "\\begin{equation}\n",
    "D_3(h) \\;:=\\; D_2(h)\\;-\\;\\frac{D_2(2h)-D_2(h)}{2^{4}-1}\n",
    "\\;=\\;\\frac{16D_2(h)-D_2(2h)}{15}. \\tag{2.7} % \n",
    "\\end{equation}\n",
    "\n",
    "In general, if $D_n$ has leading error $O(h^{2n})$, then\n",
    "\\begin{equation}\n",
    "D_{n+1}(h) \\;:=\\; D_n(h)\\;-\\;\\frac{D_n(2h)-D_n(h)}{2^{2n}-1}\n",
    "\\;=\\;\\frac{2^{2n}D_n(h)-D_n(2h)}{2^{2n}-1}. \\tag{2.8} % \n",
    "\\end{equation}\n",
    "\n",
    "*Computational layout*\n",
    "\n",
    "A convenient triangular table computes higher columns using previously computed entries; only one new base evaluation $D_1$ per row is needed. Four or five columns typically suffice before round-off dominates. \n",
    "\n",
    "\\begin{center}\n",
    "\\renewcommand{\\arraystretch}{1.3}\n",
    "\\begin{tabular}{lcccc}\n",
    "$h$ & $D_1$ & $D_2$ & $D_3$ & $\\cdots$ \\\\\n",
    "\\hline\n",
    "$0.4$ & $D_1(0.4)$ & & & \\\\\n",
    "$0.2$ & $D_1(0.2)$ & $\\displaystyle \\frac{4D_1(0.2)-D_1(0.4)}{3}$ & & \\\\\n",
    "$0.1$ & $D_1(0.1)$ & $\\displaystyle \\frac{4D_1(0.1)-D_1(0.2)}{3}$ &\n",
    "$\\displaystyle \\frac{16D_2(0.1)-D_2(0.2)}{15}$ & \\\\\n",
    "$0.05$ & $D_1(0.05)$ & $\\displaystyle \\frac{4D_1(0.05)-D_1(0.1)}{3}$ &\n",
    "$\\displaystyle \\frac{16D_2(0.05)-D_2(0.1)}{15}$ &\n",
    "$\\displaystyle \\frac{64D_3(0.05)-D_3(0.1)}{63}$ \\\\\n",
    "$\\vdots$ & $\\vdots$ & $\\vdots$ & $\\vdots$ & \n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "\\noindent\\textit{Practical notes.} Each new row introduces only one new $D_1(h)$ evaluation (at the halved step). Progressing across the row uses \\eqref{2.8} to refine the estimate and suppress successive even-power error terms until floating-point round-off limits further improvement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step sizes h: [0.4    0.2    0.1    0.05   0.025  0.0125]\n",
      "\n",
      "Richardson table (level 0 is base central-difference):\n",
      "Level 0: [23.16346429 22.41416066 22.22878688 22.18256486 22.17101693 22.16813042]\n",
      "Level 1: [22.16439278 22.16699562 22.16715752 22.16716762 22.16716825]\n",
      "Level 2: [22.16716914 22.16716831 22.1671683  22.1671683 ]\n",
      "Level 3: [22.1671683 22.1671683 22.1671683]\n",
      "Level 4: [22.1671683 22.1671683]\n",
      "Level 5: [22.1671683]\n",
      "\n",
      "Best estimate: 22.167168296792223\n",
      "True value   : 22.16716829679195\n",
      "Abs error    : 2.7355895326763857e-13\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class RichardsonResult:\n",
    "    \"\"\"Holds the full Richardson table and the best estimate.\"\"\"\n",
    "    h0: float                 # initial step\n",
    "    base_order: int           # leading error order p (e.g., 2 for central difference)\n",
    "    table: list               # table[k][n] with k=extrapolation level, n=row index\n",
    "    hs: np.ndarray            # step sizes used for base evaluations (h0/2**n)\n",
    "    best: float               # best (most extrapolated) estimate\n",
    "    best_level: int           # column index (extrapolation level) of 'best'\n",
    "    best_row: int             # row index (corresponds to smallest h)\n",
    "\n",
    "def central_difference(f, x, h):\n",
    "    \"\"\"Three-point central-difference approximation for f'(x). Error ~ O(h^2).\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2.0 * h)\n",
    "\n",
    "def richardson_extrapolate_sequence(base_vals, p):\n",
    "    \"\"\"\n",
    "    Generic Richardson extrapolation on a sequence of approximations whose leading error ~ h^p.\n",
    "    Assumes entries correspond to step sizes h, h/2, h/4, ... (i.e., geometric halving).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_vals : list or 1D array\n",
    "        A[0,n] = base approximation at h / 2**n, n = 0..N\n",
    "    p : int\n",
    "        Leading error order of the base method (e.g., 2 for central difference, 2 for trapezoid).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    table : list of lists\n",
    "        table[k][n] is the k-th extrapolation level using rows up to n (0-indexed).\n",
    "        Shape is triangular: level k has length N-k+1.\n",
    "    \"\"\"\n",
    "    base_vals = np.asarray(base_vals, dtype=float)\n",
    "    N = len(base_vals) - 1\n",
    "    table = [base_vals.copy()]  # level 0\n",
    "\n",
    "    # Build Romberg-style Richardson table:\n",
    "    # A[k][n] = (2^(p*k) * A[k-1][n] - A[k-1][n-1]) / (2^(p*k) - 1)\n",
    "    for k in range(1, N + 1):\n",
    "        prev = table[k - 1]\n",
    "        factor = 2 ** (p * k)\n",
    "        curr = (factor * prev[1:] - prev[:-1]) / (factor - 1.0)\n",
    "        table.append(curr)\n",
    "    return table\n",
    "\n",
    "def richardson_derivative(\n",
    "    f, x, h0=0.4, rows=4, max_levels=None, tol=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute f'(x) with Richardson extrapolation using central differences as base.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function f(x).\n",
    "    x : float\n",
    "        Point at which to differentiate.\n",
    "    h0 : float, optional\n",
    "        Initial step size for the first row (then halved each row). Default 0.4.\n",
    "    rows : int, optional\n",
    "        Number of base rows (evaluations at h0/2**n). >= 2 recommended. Default 4.\n",
    "    max_levels : int or None\n",
    "        Max extrapolation depth (columns). If None, uses rows-1.\n",
    "    tol : float or None\n",
    "        If provided, the function will pick the most extrapolated value whose change from\n",
    "        the previous level is < tol (using the last row). If None, it returns the most extrapolated.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    RichardsonResult\n",
    "    \"\"\"\n",
    "    if rows < 2:\n",
    "        raise ValueError(\"rows must be >= 2\")\n",
    "    if max_levels is None:\n",
    "        max_levels = rows - 1\n",
    "\n",
    "    # Step sizes: h_n = h0 / 2**n\n",
    "    hs = h0 / (2.0 ** np.arange(rows))\n",
    "\n",
    "    # Base approximations (central difference) at decreasing h\n",
    "    base_vals = [central_difference(f, x, h) for h in hs]\n",
    "\n",
    "    # Perform generic Richardson extrapolation with p=2 (central difference error ~ O(h^2))\n",
    "    p = 2\n",
    "    full_table = richardson_extrapolate_sequence(base_vals, p)\n",
    "\n",
    "    # Truncate to max_levels\n",
    "    full_table = full_table[: max_levels + 1]  # levels 0..max_levels\n",
    "\n",
    "    # Choose “best” estimate:\n",
    "    # default: last column, last row (most extrapolated with smallest h)\n",
    "    best_level = len(full_table) - 1\n",
    "    best_row = len(full_table[best_level]) - 1\n",
    "    best = full_table[best_level][best_row]\n",
    "\n",
    "    # Optional tolerance-based early pick (using last row entries across levels)\n",
    "    if tol is not None and best_level > 0:\n",
    "        # Compare successive levels at the smallest h (i.e., last row per level)\n",
    "        # Find the first level where improvement is within tol\n",
    "        col_vals = [full_table[k][-1] for k in range(len(full_table))]\n",
    "        for k in range(1, len(col_vals)):\n",
    "            if abs(col_vals[k] - col_vals[k - 1]) < tol:\n",
    "                best_level = k\n",
    "                best_row = len(full_table[k]) - 1\n",
    "                best = col_vals[k]\n",
    "                break\n",
    "\n",
    "    return RichardsonResult(\n",
    "        h0=h0,\n",
    "        base_order=p,\n",
    "        table=[np.array(col) for col in full_table],\n",
    "        hs=hs,\n",
    "        best=best,\n",
    "        best_level=best_level,\n",
    "        best_row=best_row,\n",
    "    )\n",
    "\n",
    "# ---------- Example usage ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: f(x) = x * exp(x); f'(x) = exp(x) * (1 + x)\n",
    "    def f(x):\n",
    "        return x * np.exp(x)\n",
    "\n",
    "    x0 = 2.0\n",
    "    true_deriv = np.exp(x0) * (1.0 + x0)\n",
    "\n",
    "    res = richardson_derivative(f, x0, h0=0.4, rows=6, max_levels=5, tol=1e-10)\n",
    "\n",
    "    # Pretty-print the Richardson table\n",
    "    print(\"Step sizes h:\", res.hs)\n",
    "    print(\"\\nRichardson table (level 0 is base central-difference):\")\n",
    "    for k, col in enumerate(res.table):\n",
    "        print(f\"Level {k}: {col}\")\n",
    "\n",
    "    print(\"\\nBest estimate:\", res.best)\n",
    "    print(\"True value   :\", true_deriv)\n",
    "    print(\"Abs error    :\", abs(res.best - true_deriv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
